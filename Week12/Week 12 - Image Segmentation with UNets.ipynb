{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todo\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import torchvision\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import transforms\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # Use GPU if you have one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image segmentation using a UNET\n",
    "\n",
    "\n",
    "Today, we will take a look at a UNET segmentation model. We will try this out on the Oxford  IIIT Pet dataset. This dataset is quite large (~800 MB) so I recommend downloading before class. \n",
    "\n",
    "NB: This notebook needs a GPU to run in a reasonable time. If you don't have one, I recommend using colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's looks at a couple samples from the data - always look at your data\n",
    "\n",
    "for j,batch in enumerate(data):\n",
    "    plt.subplot(2,5,j+1)\n",
    "    plt.imshow(batch[1].transpose(0,2)) #to tensor normalises images by default, but these are labels, so rescaling may be needed\n",
    "    plt.axis('off')\n",
    "    plt.colorbar(orientation='horizontal')\n",
    "    plt.subplot(2,5,j+6)\n",
    "    plt.imshow(batch[0].transpose(0,2))\n",
    "    plt.axis('off')\n",
    "    \n",
    "    if j > 3:\n",
    "        break\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((256,256))\n",
    "])\n",
    "\n",
    "def process_labels(label):\n",
    "    # You'll put some code in here later.\n",
    "    new_label = label\n",
    "    return new_label\n",
    "\n",
    "target_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((256,256)),\n",
    "    process_labels,\n",
    "])\n",
    "\n",
    "\n",
    "data = torchvision.datasets.OxfordIIITPet('./data/',download=True, split=\"trainval\", target_types=\"segmentation\",transform=transform, target_transform=target_transform)\n",
    "test_data = torchvision.datasets.OxfordIIITPet('./data/',download=True, split=\"test\", target_types=\"segmentation\",transform=transform, target_transform=target_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the data is labelled using a trimap - 3 classes, background, foreground and unknown. Let's get a pre-existing U-NET model to use.\n",
    "\n",
    "Uh oh. I couldn't find one already trained for this type of problem. What can we do? \n",
    "\n",
    "Let's try to fine-tune a model trained for something else instead!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pretrained-backbones-unet\n",
    "import backbones_unet\n",
    "from backbones_unet.model.unet import Unet\n",
    "\n",
    "print(backbones_unet.__available_models__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I don't know who [mberkay0](https://github.com/mberkay0/pretrained-backbones-unet) is, but I think it's pretty cool they released these model weights online! let's check out their model. It's likely you don't have a great GPU on hand so lets choose the atto model - that sounds small. This repository provides a number of Unet architectures with encoders (not the entire network) pre-trained on the Imagenet1k dataset. Imagenet1k (ILSVRC 2012) is an object recognition dataset with 1000 object classes, and 1.2M training images.\n",
    "\n",
    "We are going to use this as the *backbone* of our unet. The core idea is the following, a model trained to do object recognition will have learned a lot about images, and it will be easier to get the model to do segmentation, than it would to train a model to do this from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Unet(\n",
    "    backbone='convnext_atto', # backbone network name\n",
    "    in_channels=3,            # input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "    num_classes=1,            # output channels (number of classes in your dataset)\n",
    "    pretrained=True,\n",
    "    preprocessing=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"Wow, this 'atto' model has %d parameters!\"%count_parameters(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity 1:\n",
    "\n",
    "Looks like a standard u-net, but there is a problem here. Can you spot it? what is the output layer set up to do here? \n",
    "* Fill in the process labels function above to adjust the dataloader to preprocess the label images to work with this architecture.\n",
    "\n",
    "Hint. How many classes are there in our dataset, and what are the output dimensions this model produces? What is the activation function at the end of the model? \n",
    "What values do the labels take on? If we wanted to train this model with a binary cross entropy loss, what would we need to do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning a neural network\n",
    "Let's blindly apply this model to an image. Obviously this doesn't work, remember, only the encoder of the unet is trained so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = batch[0]\n",
    "print(\"Note that torch likes images to be channels first\",im.shape,\"so we will need to do some transposing to display.\") \n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(batch[0].transpose(2,0))\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(batch[1].transpose(2,0))\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(model(im[np.newaxis,:,:,:].to(device)).detach().cpu().squeeze().numpy().T)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But, it probably has learned a lot about generally applicable image features. Let's *fine tune* it to work in this context. To do this, we're just going to carry on training on the Oxford Pet dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimiser = torch.optim.AdamW(params,lr=1e-4) #Set up an optimiser\n",
    "\n",
    "train_loader = DataLoader(data,batch_size=64,shuffle=True) \n",
    "test_loader = DataLoader(data,batch_size=8,shuffle=True) \n",
    "model.train()\n",
    "\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss() # We'll use a pixel-wise Binary classification loss\n",
    "\n",
    "epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for j in range(epochs):\n",
    "    \n",
    "    batch_losses = []\n",
    "    for batch in train_loader:\n",
    "        ims,labels = batch\n",
    "        \n",
    "        output_pred = model(ims.to(device))\n",
    "        \n",
    "        loss = loss_fn(output_pred,labels.to(device))\n",
    "        \n",
    "        optimiser.zero_grad() # Cancel out gradients from previous optimiser steps\n",
    "        loss.backward() # Compute gradient of output with respect to loss\n",
    "        \n",
    "        optimiser.step() # nudge the model paramters in a sensible direction to reduce the loss\n",
    "        \n",
    "        batch_losses.append(loss.item())\n",
    "        \n",
    "    print(\"Loss at epoch %d: %f\"%(j,np.mean(batch_losses)))\n",
    "    # Lets see how it does on a few samples from our test set after each epoch\n",
    "    batch  = next(iter(test_loader))\n",
    "    pred = model(batch[0].to(device)).detach().cpu().squeeze()\n",
    "            \n",
    "    for i in range(8):\n",
    "            # How does it do now?\n",
    "            plt.subplot(3,8,1+i)\n",
    "            plt.imshow(batch[0][i,:,:,:].transpose(2,0))\n",
    "            plt.axis('off')\n",
    "            plt.subplot(3,8,9+i)\n",
    "            plt.imshow(batch[1][i,:,:,:].transpose(2,0),vmin=0,vmax=1)\n",
    "            plt.axis('off')\n",
    "            plt.colorbar(orientation='horizontal')\n",
    "            plt.subplot(3,8,17+i)\n",
    "            plt.imshow(pred[i,:,:].T,vmin=0,vmax=1)\n",
    "            plt.axis('off')\n",
    "            plt.colorbar(orientation='horizontal')\n",
    "    plt.subplots_adjust(hspace=0,wspace=0.2)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 2\n",
    "\n",
    "Cool, that seems to have worked quite well. Now lets compare against training from scratch. \n",
    "* Go back and adjust the pre-trained parameter in the model definition to false, and try this again. \n",
    "* Take a note of the loss values to compare, along with the qualitative results.\n",
    "* How does it do? \n",
    "\n",
    "Fine tuning is great, we start from a sensible initialisation and carry on training - usually we can get away with less training data this way. If you have enough data and compute though, it may be better to train from scratch though."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
